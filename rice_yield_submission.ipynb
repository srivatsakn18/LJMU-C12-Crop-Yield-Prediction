{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('rice_yield_final_sheet.xlsx')\n",
    "data = data[(data['Yield'] <= 4.5) & (data['Yield'] >= 1.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the data\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "\n",
    "sample=data[[ 'Maxtemp','Mintemp','Avgtemp']]\n",
    "axes[0, 0].boxplot(sample.values, vert=True)\n",
    "axes[0, 0].set_xticklabels(['Maximum\\nTemperature', 'Minimum\\nTemperature', 'Average\\nTemperature'], fontsize=14)\n",
    "axes[0, 0].set_title(\"Temperatures\", fontsize=14)\n",
    "\n",
    "sample=data[[ 'Humid', 'Soil_Moisture']]\n",
    "axes[0, 1].boxplot(sample.values, vert=True, labels=sample.columns)\n",
    "axes[0, 1].set_xticklabels(['Humidity', 'Soil Moisture'], fontsize=14)\n",
    "axes[0, 1].set_title(\"Humidity\", fontsize=14)\n",
    "\n",
    "sample=data[[ 'Rainfall']]\n",
    "axes[1, 0].boxplot(sample.values, vert=True, labels=sample.columns)\n",
    "axes[1, 0].set_xticklabels(['Rainfall'], fontsize=14)\n",
    "axes[1, 0].set_title('Rainfall', fontsize=14)\n",
    "\n",
    "sample=data[[ 'Evapotranspiration']]\n",
    "axes[1, 1].boxplot(sample.values, vert=True, labels=sample.columns)\n",
    "axes[1, 1].set_xticklabels(['Evapotranspiration'], fontsize=14)\n",
    "axes[1, 1].set_title('Evapotranspiration', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Rice_Visualized_Data.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Outliers\n",
    "sample = data['Evapotranspiration'].values\n",
    "\n",
    "Q1 = np.percentile(sample, 25)\n",
    "Q3 = np.percentile(sample, 75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_whisker = Q1 - 1.5 * IQR\n",
    "upper_whisker = Q3 + 1.5 * IQR\n",
    "\n",
    "data['Evapotranspiration'] = data['Evapotranspiration'].apply(lambda x: lower_whisker if x < lower_whisker else x)\n",
    "\n",
    "sample = data['Soil_Moisture'].values\n",
    "\n",
    "Q1 = np.percentile(sample, 25)\n",
    "Q3 = np.percentile(sample, 75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_whisker = Q1 - 1.5 * IQR\n",
    "upper_whisker = Q3 + 1.5 * IQR\n",
    "\n",
    "data['Soil_Moisture'] = data['Soil_Moisture'].apply(lambda x: lower_whisker if x < lower_whisker else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['Rainfall', 'Evapotranspiration', 'Soil_Moisture', 'Maxtemp','Mintemp','Avgtemp','Humid']]\n",
    "y = data['Yield']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "result_df = pd.DataFrame(columns=['Model', 'RMSE', 'MSE', 'R2'])\n",
    "\n",
    "#Linear Model\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_lr = model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "test_r2 = r2_score(y_test, y_pred_lr)\n",
    "mse = mean_squared_error(y_test, y_pred_lr)\n",
    "mae = mean_absolute_error(y_test, y_pred_lr)\n",
    "\n",
    "new_data = {'Model': 'Linear Regression', 'MAE': mae,'RMSE': test_rmse, 'MSE': mse, 'R2': test_r2}\n",
    "result_df = result_df.append(new_data, ignore_index=True)\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_knn))\n",
    "mse = mean_squared_error(y_test, y_pred_knn)\n",
    "r2 = r2_score(y_test, y_pred_knn)\n",
    "mae = mean_absolute_error(y_test, y_pred_knn)\n",
    "\n",
    "new_data = {'Model': 'K-Nearest Neighbor', 'MAE': mae,'RMSE': test_rmse, 'MSE': mse, 'R2': r2}\n",
    "result_df = result_df.append(new_data, ignore_index=True)\n",
    "\n",
    "\n",
    "#Decision Tree\n",
    "dtr = DecisionTreeRegressor(random_state=42)\n",
    "dtr.fit(X_train, y_train)\n",
    "y_pred_dtr = dtr.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_dtr))\n",
    "mse = mean_squared_error(y_test, y_pred_dtr)\n",
    "r2 = r2_score(y_test, y_pred_dtr)\n",
    "mae = mean_absolute_error(y_test, y_pred_dtr)\n",
    "\n",
    "new_data = {'Model': 'Decision Tree', 'MAE': mae,'RMSE': test_rmse, 'MSE': mse, 'R2': r2}\n",
    "result_df = result_df.append(new_data, ignore_index=True)\n",
    "\n",
    "#Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=20)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mse = mean_squared_error(y_test, y_pred_rf)\n",
    "r2 = r2_score(y_test, y_pred_rf)\n",
    "mae = mean_absolute_error(y_test, y_pred_rf)\n",
    "\n",
    "new_data = {'Model': 'Random Forest', 'MAE': mae,'RMSE': test_rmse, 'MSE': mse, 'R2': r2}\n",
    "result_df = result_df.append(new_data, ignore_index=True)\n",
    "\n",
    "#eXtreme Gradient Boosting\n",
    "xgb = XGBRegressor(n_estimators= 100, max_depth= 3, learning_rate= 0.15, subsample= 0.7, colsample_bytree= 0.9,\n",
    "    objective= 'reg:squarederror', random_state= 42)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xbg = xgb.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_xbg))\n",
    "mse = mean_squared_error(y_test, y_pred_xbg)\n",
    "r2 = r2_score(y_test, y_pred_xbg)\n",
    "mae = mean_absolute_error(y_test, y_pred_xbg)\n",
    "\n",
    "new_data = {'Model': 'Extreme Gradient Boosting', 'MAE': mae,'RMSE': test_rmse, 'MSE': mse, 'R2': r2}\n",
    "result_df = result_df.append(new_data, ignore_index=True)\n",
    "\n",
    "\n",
    "#Gradient Boosting Regressor\n",
    "gbdt = GradientBoostingRegressor(n_estimators=300, learning_rate=0.01, subsample= 0.9, \n",
    "                                 max_depth=5, min_samples_leaf= 4, random_state=42)\n",
    "gbdt.fit(X_train, y_train)\n",
    "y_pred_gdbt = gbdt.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_gdbt))\n",
    "mse = mean_squared_error(y_test, y_pred_gdbt)\n",
    "r2 = r2_score(y_test, y_pred_gdbt)\n",
    "mae = mean_absolute_error(y_test, y_pred_gdbt)\n",
    "\n",
    "new_data = {'Model': ' Gradient Boosting Regressor', 'MAE': mae,'RMSE': test_rmse, 'MSE': mse, 'R2': r2}\n",
    "result_df = result_df.append(new_data, ignore_index=True)\n",
    "\n",
    "#Support Vector Regressor\n",
    "svr_rbf = SVR(kernel='rbf', C=10, gamma='scale', epsilon=.1, degree=2)\n",
    "svr_rbf.fit(X_train, y_train)\n",
    "y_pred_svr = svr_rbf.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_svr))\n",
    "mse = mean_squared_error(y_test, y_pred_svr)\n",
    "r2 = r2_score(y_test, y_pred_svr)\n",
    "mae = mean_absolute_error(y_test, y_pred_svr)\n",
    "\n",
    "new_data = {'Model': 'Support Vector Regression', 'MAE': mae,'RMSE': test_rmse, 'MSE': mse, 'R2': r2}\n",
    "result_df = result_df.append(new_data, ignore_index=True)\n",
    "\n",
    "df_sorted = result_df.sort_values(by='R2')\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN\n",
    "class R2Callback(Callback):\n",
    "    def __init__(self, training_data, validation_data):\n",
    "        self.x_train, self.y_train = training_data\n",
    "        self.x_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred_train = self.model.predict(self.x_train)\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "\n",
    "        r2_val = r2_score(self.y_val, y_pred_val)\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_val, y_pred_val))\n",
    "        mse = mean_squared_error(self.y_val, y_pred_val)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: RÂ²: {r2_val:.6f}, RMSE: {rmse:.6f}, MSE: {mse:.6f},\")\n",
    "\n",
    "\n",
    "r2_callback = R2Callback(training_data=(X_train, y_train), validation_data=(X_test, y_test))\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=64, validation_split=0.1, verbose=1, validation_data=(X_test, y_test), callbacks=[r2_callback])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "class R2Callback(Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super(R2Callback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        X_val, y_val = self.validation_data\n",
    "        y_pred = self.model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f\"\\nEpoch {epoch+1}: RÂ² = {r2:.6f}; mse = {mse:.6f}; rmse = {rmse:.6f}\")\n",
    "\n",
    "# Step 3: Define and compile the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "r2_callback = R2Callback(validation_data=(X_test, y_test))\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), batch_size=1, callbacks=[r2_callback])\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Loss: {loss}')\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['RMSE'] = result_df['RMSE'].round(6)\n",
    "result_df['MSE'] = result_df['MSE'].round(6)\n",
    "result_df['R2'] = result_df['R2'].round(6)\n",
    "result_df['MAE'] = result_df['MAE'].round(6)\n",
    "result_df\n",
    "\n",
    "result_df.to_csv('rice_yield_final_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF-GBDT-SVR Ensemble Regressor\n",
    "base_models = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42, max_depth=20)),\n",
    "    ('gbdt', GradientBoostingRegressor(n_estimators=300, learning_rate=0.01, subsample= 0.9,max_depth=5, min_samples_leaf= 4, random_state=42)),\n",
    "    ('svr', SVR(kernel='rbf', C=10, gamma='scale', epsilon=.1, degree=2))\n",
    "]\n",
    "\n",
    "stacking_regressor = StackingRegressor(estimators=base_models)\n",
    "\n",
    "stacking_regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = stacking_regressor.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean Absolute score: {mae}')\n",
    "print(f'R2 score: {r2}')\n",
    "print(f'RMSE score: {test_rmse}')\n",
    "print(f'MSE score: {mse}')\n",
    "\n",
    "X_cv = scaler.fit_transform(X)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = cross_val_score(stacking_regressor, X_cv, y, cv=kf)\n",
    "print(f\"Cross-validated scores: {cv_scores}\")\n",
    "print(f\"Mean CV score: {np.mean(cv_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance Evaluvation\n",
    "importance = rf_model.feature_importances_\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importance\n",
    "})\n",
    "\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Parameters for Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [10, 15, 20]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error with the best parameters: {mse}\")\n",
    "print(f\"R2 score with the best parameters: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Parameters for SVR\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svr__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'svr__C': [0.1, 1, 10, 100],\n",
    "    'svr__gamma': ['scale', 'auto'],\n",
    "    'svr__degree': [2, 3, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error with the best parameters: {mse}\")\n",
    "print(f\"R2 score with the best parameters: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Parameters for XGB\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300,400,500],\n",
    "    'max_depth': [3, 4, 5,6,7,8],\n",
    "    'learning_rate': [0.1,0.15,0.2],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error with the best parameters: {mse}\")\n",
    "print(f\"R2 score with the best parameters: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Parameters for GBDT\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error with the best parameters: {mse}\")\n",
    "print(f\"R2 score with the best parameters: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Matrix\n",
    "corr_matrix = data[['Rainfall', 'Evapotranspiration', 'Soil_Moisture', 'Maxtemp','Mintemp','Avgtemp','Humid','Yield']].corr()\n",
    "\n",
    "new_column_names = {\n",
    "    'Rainfall': 'Rainfall',\n",
    "    'Evapotranspiration': 'Evapotranspiration',\n",
    "    'Soil_Moisture': 'Soil Moisture',\n",
    "    'Maxtemp': 'Maximum Temperature',\n",
    "    'Mintemp': 'Minimum Temperature',\n",
    "    'Avgtemp': 'Average Temperature',\n",
    "    'Humid': 'Humidity',\n",
    "    'Yield': 'Rice Yield'\n",
    "    \n",
    "}\n",
    "\n",
    "corr_matrix.rename(columns=new_column_names, index=new_column_names, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, square=True, linewidths=0.5)\n",
    "\n",
    "plt.title('Heatmap of Dependant variables and Rice Yield Correlation')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('Rice_Yield_Correlation.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
